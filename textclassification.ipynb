{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-16T09:18:14.048265Z","iopub.execute_input":"2023-04-16T09:18:14.048543Z","iopub.status.idle":"2023-04-16T09:18:14.083198Z","shell.execute_reply.started":"2023-04-16T09:18:14.048509Z","shell.execute_reply":"2023-04-16T09:18:14.082273Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/uta-datathon-2023-text-classification/checkworthy_labeled.csv\n/kaggle/input/uta-datathon-2023-text-classification/checkworthy_eval.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"labeled = pd.read_csv('/kaggle/input/uta-datathon-2023-text-classification/checkworthy_labeled.csv')\neval = pd.read_csv('/kaggle/input/uta-datathon-2023-text-classification/checkworthy_eval.csv')\neval.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T09:18:17.589965Z","iopub.execute_input":"2023-04-16T09:18:17.592424Z","iopub.status.idle":"2023-04-16T09:18:17.728266Z","shell.execute_reply.started":"2023-04-16T09:18:17.592369Z","shell.execute_reply":"2023-04-16T09:18:17.726486Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    Id                                               Text\n0   26      You know, I saw a movie - \"Crocodile Dundee.\"\n1   80  We're consuming 50 percent of the world's coca...\n2  129   That answer was about as clear as Boston harbor.\n3  131                          Let me help the governor.\n4  172  We've run up more debt in the last eight years...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26</td>\n      <td>You know, I saw a movie - \"Crocodile Dundee.\"</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>80</td>\n      <td>We're consuming 50 percent of the world's coca...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>129</td>\n      <td>That answer was about as clear as Boston harbor.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>131</td>\n      <td>Let me help the governor.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>172</td>\n      <td>We've run up more debt in the last eight years...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegressionCV, SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport matplotlib.pyplot as plt\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_df, test_df = train_test_split(labeled, test_size=0.1, shuffle=True)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True)\n\nvectorizer = CountVectorizer(stop_words='english', lowercase=True, analyzer='word')\nX_train = vectorizer.fit_transform(train_df['Text'])\ny_train = train_df['Category']\n\nX_val = vectorizer.transform(val_df['Text'])\ny_val, y_test = val_df['Category'], test_df['Category']\nX_test = vectorizer.transform(test_df['Text'])\ntrain = vectorizer.transform(labeled['Text'])\noutput = labeled['Category']\n\nLog_reg = LogisticRegressionCV(max_iter=100)\nLog_reg.fit(X_train, y_train)\ny_pred = Log_reg.predict(X_val)\nprint(accuracy_score(y_pred, y_val))\n\ny_pred = Log_reg.predict(X_test)\nprint(accuracy_score(y_pred, y_test))\ndef factcheck_worthy(sentence):\n    X = vectorizer.transform([sentence])\n    category = Log_reg.predict_proba(X)[:, 1][0]\n    return category\narr = []\nbest_count = 0\nfor j in range(370, 400, 1):\n    j = j/1000\n    print(j)\n    count = 0\n    for i in range(len(labeled)):\n        predict = 'No' if factcheck_worthy(labeled['Text'][i])<j else 'Yes'\n        if predict==labeled['Category'][i]:\n            count+=1\n    arr.append(count)\n    print(count)\n    best_count = max(count, best_count)\nplt.plot(arr)\nplt.show()\nprint(best_count)\n#print(random.getstate())\nstate = random.getstate()\neval['Category']=None\nfor i in range(len(eval)):\n    eval['Category'][i] = 0 if factcheck_worthy(eval['Text'][i])<0.4 else 1\ndisplay(eval['Category'].value_counts())\neval['Category'] = eval['Category'].map({0:'No',1:'Yes'})\neval[['Id','Category']].to_csv('1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T10:09:41.109836Z","iopub.execute_input":"2023-04-16T10:09:41.110369Z","iopub.status.idle":"2023-04-16T10:10:05.494583Z","shell.execute_reply.started":"2023-04-16T10:09:41.110331Z","shell.execute_reply":"2023-04-16T10:10:05.492948Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"0.8328395061728395\n0.8169702354509107\n0.37\n19718\n0.371\n19723\n0.372\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4015862245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'No'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfactcheck_worthy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'Yes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/4015862245.py\u001b[0m in \u001b[0;36mfactcheck_worthy\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfactcheck_worthy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         )\n\u001b[1;32m   1246\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     66\u001b[0m                     idx_dtype = get_index_dtype((indices, indptr),\n\u001b[1;32m     67\u001b[0m                                                 \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                                 check_contents=True)\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     self.indices = np.array(indices, copy=copy,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mget_index_dtype\u001b[0;34m(arrays, maxval, check_contents)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmaxval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mint32max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"eval['Category']=None\nfor i in range(len(eval)):\n    eval['Category'][i] = 0 if factcheck_worthy(eval['Text'][i])<0.4 else 1\ndisplay(eval['Category'].value_counts())\neval['Category'] = eval['Category'].map({0:'No',1:'Yes'})\neval[['Id','Category']].to_csv('1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegressionCV, SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport matplotlib.pyplot as plt\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nval_score, test_score = 0, 0\ndef run():\n    train_df, test_df = train_test_split(labeled, test_size=0.2, shuffle=True)\n    train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True)\n\n    vectorizer = CountVectorizer(stop_words='english', lowercase=True, analyzer='word')\n    X_train = vectorizer.fit_transform(train_df['Text'])\n    y_train = train_df['Category']\n\n    X_val = vectorizer.transform(val_df['Text'])\n    y_val, y_test = val_df['Category'], test_df['Category']\n    X_test = vectorizer.transform(test_df['Text'])\n    train = vectorizer.transform(labeled['Text'])\n    output = labeled['Category']\n    Log_reg = LogisticRegressionCV(max_iter=10)\n    Log_reg.fit(X_train, y_train)\n    y_pred = Log_reg.predict(X_val)\n    print(accuracy_score(y_pred, y_val))\n    val_score = accuracy_score(y_pred, y_val)\n\n    y_pred = Log_reg.predict(X_test)\n    print(accuracy_score(y_pred, y_test))\n    test_score = accuracy_score(y_pred, y_test)\n    return val_score, test_score\ndef factcheck_worthy(sentence):\n    X = vectorizer.transform([sentence])\n    category = Log_reg.predict_proba(X)[:, 1][0]\n    return category\n'''\narr = []\nbest_count = 0\nfor j in range(370, 400, 1):\n    j = j/1000\n    print(j)\n    count = 0\n    for i in range(len(labeled)):\n        predict = 'No' if factcheck_worthy(labeled['Text'][i])<j else 'Yes'\n        if predict==labeled['Category'][i]:\n            count+=1\n    arr.append(count)\n    print(count)\n    best_count = max(count, best_count)\nplt.plot(arr)\nplt.show()\nprint(best_count)\n#print(random.getstate())\nstate = random.getstate()\n'''\nwhile test_score<0.84 and val_score<0.84:\n    val_score,\n    test_score = run()\nprint(test_score, val_score, 'wekfjbwf')","metadata":{},"execution_count":null,"outputs":[]}]}